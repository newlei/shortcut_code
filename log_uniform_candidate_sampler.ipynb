{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "log_uniform_candidate_sampler.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMcewyGWDlyplb7WwML8n3z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/newlei/shortcut_code/blob/main/log_uniform_candidate_sampler.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SeVPy6WXIkQj",
        "outputId": "fd94c041-06c4-4c13-c5b1-b4f88253cdca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "P(class) [0.3154648767857287, 0.18453512321427132, 0.13092975357145736, 0.10155700678750612, 0.08297811642676516, 0.0701569977949824, 0.06077275577647497, 0.053605369642814]\n",
            "------------num_sampled==1---------------\n",
            "[[0.31546488]] [0.08297811] [4]\n",
            "[[0.31546488]] [0.18453512] [1]\n",
            "[[0.31546488]] [0.18453512] [1]\n",
            "[[0.31546488]] [0.18453512] [1]\n",
            "[[0.31546488]] [0.08297811] [4]\n",
            "[[0.31546488]] [0.31546488] [0]\n",
            "[[0.31546488]] [0.13092975] [2]\n",
            "[[0.31546488]] [0.18453512] [1]\n",
            "[[0.31546488]] [0.13092975] [2]\n",
            "[[0.31546488]] [0.13092975] [2]\n",
            "------------num_sampled==2---------------\n",
            "[[0.63092977]] [0.36907023 0.2618595 ] [1 2]\n",
            "[[0.63092977]] [0.16595623 0.36907023] [4 1]\n",
            "[[0.63092977]] [0.2618595  0.63092977] [2 0]\n",
            "[[0.63092977]] [0.140314   0.20311402] [5 3]\n",
            "[[0.63092977]] [0.63092977 0.36907023] [0 1]\n",
            "[[0.63092977]] [0.16595623 0.140314  ] [4 5]\n",
            "[[0.63092977]] [0.36907023 0.2618595 ] [1 2]\n",
            "[[0.63092977]] [0.16595623 0.20311402] [4 3]\n",
            "[[0.63092977]] [0.2618595  0.63092977] [2 0]\n",
            "[[0.67923486]] [0.34360594 0.1960503 ] [2 5]\n",
            "------------num_sampled==3---------------\n",
            "[[0.8496932]] [0.6393995  0.26910767 0.8496932 ] [1 6 0]\n",
            "[[0.78042495]] [0.22181523 0.5577977  0.252453  ] [6 1 5]\n",
            "[[0.9463947]] [0.30467102 0.24893434 0.9463947 ] [3 4 0]\n",
            "[[0.9463947]] [0.30467102 0.5536053  0.9463947 ] [3 1 0]\n",
            "[[0.9463947]] [0.24893434 0.39278924 0.9463947 ] [4 2 0]\n",
            "[[0.8496932]] [0.5042367 0.6393995 0.8496932] [2 1 0]\n",
            "[[0.9463947]] [0.24893434 0.9463947  0.30467102] [4 0 3]\n",
            "[[0.78042495]] [0.3484285  0.42954746 0.5577977 ] [3 2 1]\n",
            "[[0.9463947]] [0.24893434 0.39278924 0.9463947 ] [4 2 0]\n",
            "[[0.78042495]] [0.29283816 0.78042495 0.5577977 ] [4 0 1]\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "tf.__version__\n",
        "import numpy as np\n",
        "\n",
        "def gen_neg(num_sampled):\n",
        "    SEED = 42 \n",
        "    context_class = tf.reshape(tf.constant(0, dtype=\"int64\"), (1, 1))#正样本id为0\n",
        "    negative_sampling_candidates, true_expected_count, sampled_expected_count = tf.random.log_uniform_candidate_sampler(\n",
        "        true_classes=context_class, # class that should be sampled as 'positive'\n",
        "        num_true=1, # each positive skip-gram has 1 positive context class\n",
        "        num_sampled=num_sampled, # number of negative context words to sample\n",
        "        unique=True, # all the negative samples should be unique\n",
        "        range_max=8, # pick index of the samples from [0, vocab_size]\n",
        "        seed=SEED, # seed for reproducibility\n",
        "        name=\"negative_sampling\" # name of this operation\n",
        "    ) \n",
        "    print(true_expected_count.numpy(),sampled_expected_count.numpy(),negative_sampling_candidates.numpy())\n",
        "\n",
        "#计算P(class)，方便对应。\n",
        "prob_list=[]\n",
        "for i in range(8): \n",
        "    pro_one = (np.log(i + 2) - np.log(i + 1)) / np.log(8 + 1) \n",
        "    prob_list.append(pro_one)\n",
        "print('P(class)',prob_list)\n",
        "\n",
        "#采样1个负样本的时候。\n",
        "print('------------num_sampled==1---------------')\n",
        "for i in range(10):\n",
        "    #可以看到true_expected_count都是P(0)=0.31546488的值。\n",
        "    #sampled_expected_count是采样得到的负样本，概率就等于对应的P(class)，就是P(negative_sampling_candidates)。\n",
        "    #采样得到的负样本negative_sampling_candidates，可以是正样本这里没有做区分，如果需要不包含正样本tf是在外层处理的。如：tf.nn.sampled_softmax_loss在采样结束后有对应的处理代码。\n",
        "    gen_neg(1)\n",
        "\n",
        "print('------------num_sampled==2---------------')\n",
        "#采样2个负样本的时候。\n",
        "for i in range(10):\n",
        "    #可以看到true_expected_count是P(0)*num_sampled=0.31546488*2=0.63092976的附近。所以存在一个波动。即：\\hat{P}(0)=[P(0)+波动]+[P(0)+波动]\n",
        "    #sampled_expected_count是采样得到的负样本，概率是P(class)*num_sampled的附近，也是有个波动的。\n",
        "    gen_neg(2)\n",
        "\n",
        "print('------------num_sampled==3---------------')\n",
        "#采样2个负样本的时候。\n",
        "for i in range(10):\n",
        "    #可以看到true_expected_count是P(0)*num_sampled=0.31546488*3=0.94639464的附近。即：\\hat{P}(0)=[P(0)+波动]+[P(0)+波动]+[P(0)+波动]。因为num_sampled变多了，波动影响也就更大。\\hat{P}(0)的浮动就大了\n",
        "    #sampled_expected_count是采样得到的负样本，概率是P(class)*num_sampled的附近，波动也就更大\n",
        "    gen_neg(3)"
      ]
    }
  ]
}